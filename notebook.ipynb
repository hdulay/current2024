{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time RAG and Agentic Analytics\n",
    "\n",
    "<img src=\"./web/pinot.png\" alt=\"drawing\" style=\"width:800px;\"/>\n",
    "\n",
    "***Getting started with vectors, similarity search, RAG, and agents in real-time analytics.***\n",
    "\n",
    "\n",
    "# Hubert Dulay - Developer Advocate @StarTree\n",
    "<img src=\"./web/sdb.jpg\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "<img src=\"./web/sdm.jpg\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "\n",
    "## What is a Vector?\n",
    "\n",
    "> A vector is an array of numbers that represents unstructured data like text and images. For example, let’s represent these sentences as vectors:\n",
    "\n",
    "\n",
    "s1 = “I love data”\n",
    "\n",
    "s2 = “I love candy”\n",
    "\n",
    "We can take all the terms and create what is called a bag of words:\n",
    "\n",
    "|ID|candy|data|I|love|\n",
    "|-|-|-|-|-|\n",
    "|s1|0|1|1|1|\n",
    "|s2|1|0|1|1|\n",
    "\n",
    "## What is an Embedding?\n",
    "\n",
    "What is an embedding?\n",
    "> \n",
    "> A vector with a large number of dimensions created by a neural network, the vectors are created by predicting for each word what its neighboring words may be.\n",
    "> \n",
    "\n",
    "How is it different from a Bag of Words (BoW)?\n",
    "> BoW rely on frequencies of words under the unrealistic assumption that each word occurs independently of all others. Example - “Good” vs “Not Good”\n",
    "\n",
    "## What is a Vector Index?\n",
    "\n",
    "> An \"index\" is a data structure that improves the speed of data retrieval operations on a database table.\n",
    "\n",
    "> A “vector index” is a mechanism that efficiently organizes and retrieves vectors based on their content. \n",
    "\n",
    "## What is a Vector Database?\n",
    "\n",
    "A vector database is a database that encompasses the features designed to manage vector data, including storage, retrieval, and query processing. It may utilize vector indexing as part of its strategy for efficient vector-oriented operations.\n",
    "\n",
    "## Image Search Demo with pg_vector\n",
    "\n",
    "What is `pg_vector`?\n",
    "> Open-source vector similarity search for Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from pgvector.psycopg import register_vector\n",
    "import os\n",
    "\n",
    "# localhost\n",
    "conn = psycopg.connect(dbname=\"postgres\", autocommit=True)\n",
    "\n",
    "# image and text embedding model\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mulitmodal Models\n",
    "\n",
    "`SentenceTransformer('clip-ViT-B-32')` is a multimodal model.\n",
    "\n",
    "Multimodal models are advanced AI systems designed to process and integrate information from multiple modalities, such as text, images, audio, and video, to generate more comprehensive and contextually relevant outputs. \n",
    "\n",
    "These models leverage diverse data types to enhance understanding and decision-making, enabling applications like image captioning, speech-to-text translation, and cross-modal retrieval. By combining different sensory inputs, multimodal models can achieve a richer, more nuanced understanding of complex scenarios, making them powerful tools for tasks that require a holistic interpretation of diverse data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up The Database\n",
    "\n",
    "We load all the images in the `mixed_wiki` directory and convert them into embeddings. Then we insert them into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database and table\n",
    "conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "register_vector(conn)\n",
    "conn.execute('DROP TABLE IF EXISTS images')\n",
    "conn.execute('CREATE TABLE images (id bigserial PRIMARY KEY, path varchar(64), embedding vector(512))')\n",
    "cur = conn.cursor()\n",
    "cur.execute('create extension if not exists vector with schema public')\n",
    "\n",
    "# load images and create embeddings\n",
    "images = os.listdir(\"./mixed_wiki\")\n",
    "for f in images:\n",
    "    file = f'./mixed_wiki/{f}'\n",
    "    img_emb = model.encode(Image.open(file))\n",
    "    cur.execute('INSERT INTO images (embedding, path) VALUES (%s,%s)', (img_emb.tolist(), file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's ask for Van Gogh paintings from the vector database. \n",
    "\n",
    "> The only way we can do this is to use the same embedding model to ensure the vectors are within the same vector space. This requires a multimodal model that can create embeddings for both text and images.\n",
    "\n",
    "> Notice below the `where` clause leveraging the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(path, distance):\n",
    "    plt.title(f'{path} {distance}')\n",
    "    image = mpimg.imread(path)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "query_string = \"Van Gogh paintings\"\n",
    "text_emb = model.encode(query_string)\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                id, \n",
    "                path, \n",
    "                embedding <-> %s AS distance \n",
    "            FROM images \n",
    "            WHERE embedding <-> %s < 15\n",
    "            ORDER BY distance\n",
    "            LIMIT 3\n",
    "            \"\"\", \n",
    "            (\n",
    "                str(text_emb.tolist())\n",
    "                ,str(text_emb.tolist())\n",
    "            )\n",
    "    )\n",
    "\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row[1], rows[2])\n",
    "    show(row[1], rows[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Algorithms\n",
    "\n",
    "Measures how close two vectors are. Supported distance functions:\n",
    "\n",
    "<-> - L2 distance (euclidean)\n",
    "\n",
    "<#> - (negative) inner product\n",
    "\n",
    "<=> - cosine distance (angle)\n",
    "\n",
    "<+> - L1 distance (manhattan)\n",
    "\n",
    "![alt](./web/distance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is RAG vs Real-Time RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) example using Apache Pinot,  LangChain, and OpenAI. The use case is to load documentation and allow an LLM to answer questions provided by a user. This approach enables you to generate AI responses that are fresh and in real time. A diagram of the data flow is shown in the Mermaid diagram below.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "Documents-->Chunk-->e[Embedding Model]\n",
    "\n",
    "\n",
    "e-.->|streaming|k[Kafka/Confluent Cloud]\n",
    "k-.->|upsert|db[(Vector Database)]\n",
    "e-->|batching|db\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "We use the same embedding model to convert the question into an embedding. We take advantage of the multimodal capabilities of the embedding model to put the question and images in the same vector space and use distance algorithms for similarity searching.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "Query-->e[Embedding Model]-->|retrieve|db[(Vector Database)]-->|context|Prompt-->LLM-->Response\n",
    "Query-->Prompt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Pinot Vector Index\n",
    "\n",
    "Apache Pinot has the same capabilities. By sending our embeddings over Kafka and into StarTree cloud (Pinot), we can take advantage of a real-time OLAP database to serve our embeddings. This provides you with better performance with low latency similarity queries and high concurrent end users.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "subgraph ie[Pinot]\n",
    "Images\n",
    "end\n",
    "\n",
    "k[Kafka]\n",
    "frame\n",
    "\n",
    "video-->frame-->|embedding stream|k-->Images\n",
    "\n",
    "Search-->Images\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with HNSW: Apache Pinot Vector Index\n",
    "\n",
    "Apache Pinot's vector index, specifically the HNSW (Hierarchical Navigable Small World) vector index, is a feature that enables efficient similarity search within high-dimensional data. It allows users to perform approximate nearest neighbor (ANN) searches, which is crucial for applications like recommendation engines, anomaly detection, and real-time machine learning inference. By leveraging this index, Pinot can quickly and accurately retrieve the most similar items in a large dataset, making it well-suited for real-time analytics and AI-driven use cases.\n",
    "\n",
    "The HNSW algorithm builds a multi-layer graph where each vector is connected to its nearest neighbors. During search, it navigates from a high-level sparse graph to a more detailed one, using a greedy approach to find approximate nearest neighbors efficiently.\n",
    "\n",
    "<img src=\"web/hnsw.png\" alt=\"drawing\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use OLAP Vector Database vs OLTP Vector Database\n",
    "- AI application needs millisecond latency for exact and similarity retrieval.\n",
    "- AI application needs to serve many concurrent users.\n",
    "- AI application needs to aggregate historical data that exceeds the capacity of an OLTP database.\n",
    "\n",
    "## When to use OLTP Vector Database vs OLAP Vector Database\n",
    "- AI application needs consistency versus eventually consistent OLAP.\n",
    "- AI application only needs to search small amounts of data.\n",
    "\n",
    "\n",
    "## NYC Cameras\n",
    "\n",
    "In NYC, there are about ~50,000 street cameras in all 5 boroughs (according to llama3). If each camera captures a frame every 10 seconds, that is 157,680,000,000 images in 1 year.\n",
    "\n",
    "<img src=\"./web/cameras.png\" alt=\"drawing\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Rigid Dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinotdb import connect\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# StarTree Cloud Broker URL and Bearer Auth Token\n",
    "PINOT_BROKER_HOST = os.getenv(\"PINOT_BROKER_HOST\")\n",
    "PINOT_USER = os.getenv(\"PINOT_USER\")\n",
    "PINOT_PASSWORD = os.getenv(\"PINOT_PASSWORD\")\n",
    "DATABASE = os.getenv('PINOT_WORKSPACE')\n",
    "\n",
    "conn = connect(host=f'{PINOT_USER}:{PINOT_PASSWORD}@{PINOT_BROKER_HOST}', \n",
    "    port=443, \n",
    "    path='/query/sql', \n",
    "    scheme='https',\n",
    "    database=DATABASE,\n",
    "    use_multistage_engine=False)\n",
    "\n",
    "curs = conn.cursor()\n",
    "sql = f\"\"\"\n",
    "    select \n",
    "        event_type, \n",
    "        count(event_type) as event \n",
    "    from clickstream\n",
    "    group by event_type\n",
    "    order by event desc\n",
    "    \"\"\"\n",
    "curs.execute(sql)\n",
    "df = pd.DataFrame(curs, columns=[item[0] for item in curs.description])\n",
    "\n",
    "import plotly.express as px\n",
    "data = dict(\n",
    "    number=df['event'].tolist(),\n",
    "    stage=['Clicks', 'Views', 'Saves', 'Purchases']\n",
    ")\n",
    "fig = px.funnel(data, x='number', y='stage')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-Generation Agentic Analytics\n",
    "\n",
    "<img src=\"./web/agent.jpg\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "An agentic AI system can \n",
    "\n",
    "- make decisions\n",
    "- take actions\n",
    "- engage with its surroundings independently, aiming to accomplish set goals or tasks. \n",
    " \n",
    "This idea of agency in AI marks a transition from `passive systems` that follow preset instructions to `active entities` that can think, act, and react independently within defined parameters. Especially within rigid and inflexible analytical dashboards.\n",
    "\n",
    "Agents execute `tasks` using `tools`. \n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "query-->Agent-->Tools-->Response\n",
    "\n",
    "```\n",
    "\n",
    "In the next-generation real-time analytics, we can build real-time tools using `Apache Pinot`, a real-time OLAP database that can query streams with `low-latency` and `high concurrency`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Build and Agent Using Pinot and LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# StarTree Cloud Broker URL and Bearer Auth Token\n",
    "PINOT_BROKER_HOST = os.getenv(\"PINOT_BROKER_HOST\")\n",
    "PINOT_USER = os.getenv(\"PINOT_USER\")\n",
    "PINOT_PASSWORD = os.getenv(\"PINOT_PASSWORD\")\n",
    "DATABASE = os.getenv('PINOT_WORKSPACE')\n",
    "\n",
    "conn2 = connect(host=f'{PINOT_USER}:{PINOT_PASSWORD}@{PINOT_BROKER_HOST}', \n",
    "    port=443, \n",
    "    path='/query/sql', \n",
    "    scheme='https',\n",
    "    database=DATABASE,\n",
    "    use_multistage_engine=True)\n",
    "\n",
    "\n",
    "dimensions = 2048\n",
    "config = {\n",
    "    \"model\": 'text-embedding-3-large',\n",
    "    \"dimensions\": int(dimensions) if dimensions is not None else None,\n",
    "}\n",
    "embed_model = OpenAIEmbedding(**config)\n",
    "\n",
    "\n",
    "def get_all_users() -> List[str]:\n",
    "    curs = conn2.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            distinct ID\n",
    "        from User\n",
    "        \"\"\"\n",
    "    top = []\n",
    "    curs.execute(sql)\n",
    "    for row in curs:\n",
    "        top.append(row[0])\n",
    "\n",
    "    return top\n",
    "\n",
    "def get_user_details(user_id:str): \n",
    "    curs = conn2.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            Address, City, ID, Name, Phone, State, ZIP\n",
    "        from User\n",
    "        where ID = '{user_id}'\n",
    "        \"\"\"\n",
    "    curs.execute(sql)\n",
    "    columns=[item[0] for item in curs.description]\n",
    "    for row in curs:\n",
    "        rec = {}\n",
    "        for i, col in enumerate(columns):\n",
    "            rec[col] = row[i]\n",
    "        return rec\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_recent_purchases(user_id:str):\n",
    "    curs = conn2.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            Name, Description\n",
    "        from Purchase p1\n",
    "        join Product p2 on p1.product_id = p2.ID\n",
    "        where p1.user_id = '{user_id}'\n",
    "        \"\"\"\n",
    "    curs.execute(sql)\n",
    "    columns=[item[0] for item in curs.description]\n",
    "    for row in curs:\n",
    "        rec = {}\n",
    "        for i, col in enumerate(columns):\n",
    "            rec[col] = row[i]\n",
    "        return rec\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_similar_products(product_description:str):\n",
    "    embedding = embed_model.get_text_embedding(product_description)\n",
    "    curs = conn2.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            Name,\n",
    "            Description,\n",
    "            cosine_distance(embedding, ARRAY{embedding}) AS cosine, \n",
    "            l2_distance(embedding, ARRAY{embedding}) AS l2, \n",
    "            l1_distance(embedding, ARRAY{embedding}) AS l1\n",
    "        from Product\n",
    "        where \n",
    "            VECTOR_SIMILARITY(embedding, ARRAY{embedding}, 3)\n",
    "        order by cosine asc\n",
    "        limit 3\n",
    "        \"\"\"\n",
    "    curs.execute(sql)\n",
    "    columns=[item[0] for item in curs.description]\n",
    "    for row in curs:\n",
    "        rec = {}\n",
    "        for i, col in enumerate(columns):\n",
    "            rec[col] = row[i]\n",
    "        return rec\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def top_three() -> List[str]:\n",
    "    curs = conn2.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            count(event_type) as purchases\n",
    "        from Clickstream\n",
    "        where event_type = 'purchase'\n",
    "        group by event_type, user_id\n",
    "        order by purchases desc\n",
    "        limit 3\n",
    "        \"\"\"\n",
    "    top = []\n",
    "    curs.execute(sql)\n",
    "    for row in curs:\n",
    "        top.append(row[0])\n",
    "\n",
    "    return top\n",
    "\n",
    "def most_time_spent() -> List[str]:\n",
    "    curs = conn2.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            user_id\n",
    "        from Clickstream\n",
    "        order by duration desc\n",
    "        limit 3\n",
    "        \"\"\"\n",
    "    top = []\n",
    "    curs.execute(sql)\n",
    "    for row in curs:\n",
    "        print(row)\n",
    "        top.append(row[0])\n",
    "\n",
    "    return top\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_three_tool = FunctionTool.from_defaults(\n",
    "    fn=top_three,\n",
    "    name=\"top_three\",\n",
    "    description=\"gets the top three purchasers in the application\",\n",
    ")\n",
    "\n",
    "most_time_spent_tool = FunctionTool.from_defaults(\n",
    "    fn=most_time_spent,\n",
    "    name=\"most_time\",\n",
    "    description=\"gets the top 3 users that have spent the most time on an application\"\n",
    ")\n",
    "\n",
    "get_all_users_tool = FunctionTool.from_defaults(\n",
    "    fn=get_all_users,\n",
    "    name=\"get_all_users\",\n",
    "    description=\"gets all the users of the application\"\n",
    ")\n",
    "\n",
    "user_info_tool = FunctionTool.from_defaults(\n",
    "    fn= get_user_details,\n",
    "    description=\"gets the details about a user\"\n",
    ")\n",
    "\n",
    "get_recent_purchases_tool = FunctionTool.from_defaults(\n",
    "    fn=get_recent_purchases,\n",
    "    name=\"get_recent_purchases\",\n",
    "    description=\"gets recent purchases for a user\"\n",
    ")\n",
    "\n",
    "get_similar_products_tool = FunctionTool.from_defaults(\n",
    "    fn=get_similar_products,\n",
    "    name=\"get_similar_products\",\n",
    "    description=\"gets similar products based on another product description\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(\n",
    "    [\n",
    "        top_three_tool, \n",
    "        most_time_spent_tool, \n",
    "        get_all_users_tool, \n",
    "        user_info_tool,\n",
    "        get_recent_purchases_tool,\n",
    "        get_similar_products_tool\n",
    "    ],\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4o-mini\"),\n",
    "    verbose=True,\n",
    "    system_prompt=\"\"\"\n",
    "You are a sales agent that is looking to get more users to spend more time \\\n",
    "in an application and ultimately provide incentives. You help this sales agent \\\n",
    "identify users of the application so that she can personally contact them \\\n",
    "with additional incentives. \\\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the Agent Questions\n",
    "\n",
    "- What product should i send to the top user?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    ask = input(\"ask:\").strip()\n",
    "    if(ask == \"quit\" or ask == \"\"):\n",
    "        break;\n",
    "    else:\n",
    "        print(agent.chat(ask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Information\n",
    "\n",
    "|Try StarTree Cloud - Free Forever|Follow Me|\n",
    "|-|-|\n",
    "|<img src=\"./web/stqr.png\" alt=\"drawing\" style=\"width:300px;\"/>| <img src=\"./web/hubertqr.png\" alt=\"drawing\" style=\"width:300px;\"/>|\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
